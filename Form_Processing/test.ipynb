{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1376 1000\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = cv2.imread('/home/amit/Shared/CTEVT/forms/person1/form.jpg')\n",
    " \n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "edges = cv2.Canny(blurred, threshold1=100, threshold2=200)\n",
    "contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "\n",
    "contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "face = False\n",
    "\n",
    "image_h, image_w = image.shape[:2]\n",
    "\n",
    "print(image_h, image_w)\n",
    "for contour in contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    gray_photo = gray[y:y+h, x:x+w]\n",
    "    faces = face_cascade.detectMultiScale(gray_photo, scaleFactor=1.1, minNeighbors=5, minSize=(50, 50))\n",
    "    print(len(faces))\n",
    "    if len(faces)>=1 and (0.07*image_w < w < 0.2*image_w and 0.07*image_h < h < 0.2*image_h):\n",
    "        passport_photo = image[y:y+h, x:x+w]\n",
    "        passport_photo = cv2.resize(passport_photo, (192, 192),interpolation=cv2.INTER_LANCZOS4)\n",
    "        face = True\n",
    "        break\n",
    "\n",
    "if not face:\n",
    "    passport_photo = np.random.randint(0, 10, (192, 192))\n",
    "\n",
    "cv2.imwrite('photo.jpg', passport_photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\form.jpg\n",
      ".\\form.jpg\\form2.jpg\n",
      ".\\form.jpg\\form2.jpg\\New folder\n",
      ".\\form.jpg\\form2.jpg\\New folder\\New folder - Copy\n",
      ".\\form.jpg\\form2.jpg\\New folder\\New folder - Copy\\New folder - Copy (2)\n",
      ".\\form.jpg\\form2.jpg\\New folder\\New folder - Copy\\New folder - Copy (2)\\New folder - Copy (3)\n",
      ".\\form.jpg\\form2.jpg\\New folder\\New folder - Copy\\New folder - Copy (2)\\New folder - Copy (3)\\photo.jpg\n",
      ".\\form.jpg\\form2.jpg\\New folder\\New folder - Copy\\New folder - Copy (2)\\New folder - Copy (3)\\photo.jpg\\test.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "path = '.'\n",
    "dirs = os.listdir(path, dir)\n",
    "\n",
    "for dir in dirs:\n",
    "    path = os.path.join(path, dir)\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_rotation(image):\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Detect edges\n",
    "    edges = cv2.Canny(blurred, 50, 150, apertureSize=3)\n",
    "\n",
    "    # Use Hough Line Transform to detect lines\n",
    "    lines = cv2.HoughLines(edges, 1, np.pi / 180, 150)\n",
    "\n",
    "    # Initialize the angle\n",
    "    angle = 0\n",
    "\n",
    "    # If lines are found, compute the median angle of the lines\n",
    "    if lines is not None:\n",
    "        angles = []\n",
    "        for rho, theta in lines[:, 0]:\n",
    "            angle_in_degrees = np.degrees(theta) - 90  # Convert from radians to degrees\n",
    "            angles.append(angle_in_degrees)\n",
    "        \n",
    "        # Use the median angle to correct the image rotation\n",
    "        angle = np.median(angles)\n",
    "\n",
    "    # Rotate the image to align it\n",
    "    height, width = image.shape[:2]\n",
    "    center = (width // 2, height // 2)\n",
    "\n",
    "    # Get the rotation matrix\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "    # Apply the rotation matrix\n",
    "    rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
    "\n",
    "    return rotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ph = cv2.imread('photo.jpg')\n",
    "c = correct_rotation(ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('rotated.jpg', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting the rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8980\\182739435.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0malign_face\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "def align_face(image):\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = detector(gray)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        # Assuming the first detected face is the one we want to align\n",
    "        face = faces[0]\n",
    "\n",
    "        # Get facial landmarks\n",
    "        landmarks = predictor(gray, face)\n",
    "\n",
    "        # Get the coordinates of the eyes (use landmarks for the eyes)\n",
    "        left_eye = (landmarks.part(36).x, landmarks.part(36).y)\n",
    "        right_eye = (landmarks.part(45).x, landmarks.part(45).y)\n",
    "\n",
    "        # Compute the angle of rotation between the eyes\n",
    "        delta_y = right_eye[1] - left_eye[1]\n",
    "        delta_x = right_eye[0] - left_eye[0]\n",
    "        angle = np.degrees(np.arctan2(delta_y, delta_x))  # Angle in degrees\n",
    "\n",
    "        # Get the center of the image\n",
    "        height, width = image.shape[:2]\n",
    "        center = (width // 2, height // 2)\n",
    "\n",
    "        # Rotation matrix to align the eyes horizontally\n",
    "        rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "\n",
    "        # Rotate the image\n",
    "        rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
    "\n",
    "        return rotated_image\n",
    "    else:\n",
    "        print(\"No face detected!\")\n",
    "        return image\n",
    "\n",
    "# Load the passport-size image\n",
    "image = cv2.imread('rotated.jpg')\n",
    "\n",
    "# Align the image (correct rotation based on face)\n",
    "aligned_image = align_face(image)\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow('Aligned Image', aligned_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Save the corrected image\n",
    "cv2.imwrite('aligned_passport_image.jpg', aligned_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\form2.jpg\n",
      ".\\photo.jpg\n",
      ".\\New_folder\\form.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Replace with the root directory you want to start the search from\n",
    "root_dir = '.'\n",
    "\n",
    "# Loop through all directories and files\n",
    "for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "        p = os.path.join(root, file)\n",
    "        if p.endswith('.jpg'):\n",
    "            print(p)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
